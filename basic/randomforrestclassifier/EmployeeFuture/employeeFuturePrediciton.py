# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mt_DSxa3h32Kj91a_DkG4cJ9kA7gjZ1a
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab import files
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier  
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, classification_report

file = files.upload()

df = pd.read_csv('Employee.csv')

df.head()

df.info()

#Cleaning the data 
df.isnull().sum()

df.describe()

df['is_duplicated'] = df.duplicated()

df['is_duplicated'].value_counts()

true_index = df.index[df['is_duplicated']]
true_index

df.drop(index = true_index, axis = 0, inplace = True)

df.shape
# now we have deleted the duplicate data

#Visualize the data
sns.pairplot(df)

sns.barplot(data = df, x = 'Age', y ='PaymentTier')

sns.relplot(data = df, x = 'Age', y = 'PaymentTier', hue = 'City')

for i in df.columns :
  sns.catplot(data = df, x =i, kind = 'count', hue = 'Gender')

sns.heatmap(df.corr(), annot = True, cmap = 'Blues', cbar= True)
# from the table there are no specific columns that has the big correlation score

# Convert string Values int Integer Values
df['Education'] = LabelEncoder().fit_transform(df['Education'])
df['Gender'] = LabelEncoder().fit_transform(df['Gender'])
df['City'] = LabelEncoder().fit_transform(df['City'])
df['EverBenched'] = LabelEncoder().fit_transform(df['EverBenched'])
#Selecting x and y dataset
x = df.drop(['is_duplicated', 'LeaveOrNot'], axis = 1)
y = df['LeaveOrNot']

#Retrieving train and test dataset
x_train, x_test, y_train, y_test = train_test_split(x,y,random_state = 10, test_size = 0.1)

#Process the Data
rfc = RandomForestClassifier()
lr = LogisticRegression()
knn = KNeighborsClassifier()
dtc = DecisionTreeClassifier()

rfc.fit(x_train, y_train)
lr.fit(x_train, y_train)
knn.fit(x_train, y_train)
dtc.fit(x_train, y_train)

rfcpred = rfc.predict(x_test)
lrpred = lr.predict(x_test)
knnpred = knn.predict(x_test)
dtcpred = dtc.predict(x_test)

rfc_accuracy = accuracy_score(rfcpred, y_test)
print("The accuracy from RandomForestClassifier {}".format(rfc_accuracy))

lr_accuracy = accuracy_score(lrpred, y_test)
print("The accuracy from LogisticRegression {}".format(lr_accuracy))

knn_accuracy = accuracy_score(knnpred, y_test)
print("The accuracy from KNN {}".format(knn_accuracy))

dtc_accuracy = accuracy_score(dtcpred, y_test)
print("The accuracy from DecisionTrees {}".format(dtc_accuracy))

cfm_rfc = confusion_matrix(rfcpred, y_test)
sns.heatmap(cfm_rfc/np.sum(cfm_rfc), annot = True, cmap = 'Blues', cbar = True)

cfm_lr = confusion_matrix(lrpred, y_test)
sns.heatmap(cfm_lr/np.sum(cfm_lr), annot = True, cmap = 'Blues', cbar = True)

cfm_knn = confusion_matrix(knnpred, y_test)
sns.heatmap(cfm_knn/np.sum(cfm_knn), annot = True, cmap = 'Blues', cbar = True)

cfm_dtc = confusion_matrix(dtcpred, y_test)
sns.heatmap(cfm_dtc/np.sum(cfm_dtc), annot = True, cmap = 'Blues', cbar = True)

